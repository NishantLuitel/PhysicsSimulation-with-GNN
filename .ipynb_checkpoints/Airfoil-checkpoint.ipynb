{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee6a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, LayerNorm, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acbde0",
   "metadata": {},
   "source": [
    "### Get the path for various directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b49254",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(root_dir, 'Airflow')\n",
    "checkpoint_dir = os.path.join(root_dir, 'best_models')\n",
    "postprocess_dir = os.path.join(root_dir, 'animations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0dc58b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_dir F:\\GNNsim2\\Airflow\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset_dir {}\".format(dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff417f",
   "metadata": {},
   "source": [
    "### Import libraries involved in loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67c02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import tensorflow.compat.v1 as tf\n",
    "import functools\n",
    "import json\n",
    "from torch_geometric.data import Data\n",
    "import enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868f14e",
   "metadata": {},
   "source": [
    "### Lets Look at the metadata about with Airfoil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a61cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root_dir, 'Airfoil/meta.json')) as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258b82dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simulator': 'su2',\n",
       " 'dt': 0.0002,\n",
       " 'collision_radius': None,\n",
       " 'features': {'node_type': {'type': 'static',\n",
       "   'shape': [1, 5233, 1],\n",
       "   'dtype': 'int32'},\n",
       "  'cells': {'type': 'static', 'shape': [1, 10216, 3], 'dtype': 'int32'},\n",
       "  'mesh_pos': {'type': 'static', 'shape': [1, 5233, 2], 'dtype': 'float32'},\n",
       "  'density': {'type': 'dynamic', 'shape': [601, 5233, 1], 'dtype': 'float32'},\n",
       "  'pressure': {'type': 'dynamic', 'shape': [601, 5233, 1], 'dtype': 'float32'},\n",
       "  'velocity': {'type': 'dynamic',\n",
       "   'shape': [601, 5233, 2],\n",
       "   'dtype': 'float32'}},\n",
       " 'field_names': ['node_type',\n",
       "  'cells',\n",
       "  'mesh_pos',\n",
       "  'density',\n",
       "  'pressure',\n",
       "  'velocity'],\n",
       " 'trajectory_length': 601}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1160d",
   "metadata": {},
   "source": [
    "### Load the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded00168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse(proto, meta):\n",
    "  \"\"\"Parses a trajectory from tf.Example.\"\"\"\n",
    "  feature_lists = {k: tf.io.VarLenFeature(tf.string)\n",
    "                   for k in meta['field_names']}\n",
    "  features = tf.io.parse_single_example(proto, feature_lists)\n",
    "  out = {}\n",
    "  for key, field in meta['features'].items():\n",
    "    data = tf.io.decode_raw(features[key].values, getattr(tf, field['dtype']))\n",
    "    data = tf.reshape(data, field['shape'])\n",
    "    if field['type'] == 'static':\n",
    "      data = tf.tile(data, [meta['trajectory_length'], 1, 1])\n",
    "    elif field['type'] == 'dynamic_varlen':\n",
    "      length = tf.io.decode_raw(features['length_'+key].values, tf.int32)\n",
    "      length = tf.reshape(length, [-1])\n",
    "      data = tf.RaggedTensor.from_row_lengths(data, row_lengths=length)\n",
    "    elif field['type'] != 'dynamic':\n",
    "      raise ValueError('invalid data format')\n",
    "    out[key] = data\n",
    "  return out\n",
    "\n",
    "\n",
    "def load_dataset(split):\n",
    "  \"\"\"Load dataset.\"\"\"\n",
    "  with open(os.path.join(root_dir, 'Airfoil/meta.json'), 'r') as fp:\n",
    "    meta = json.loads(fp.read())\n",
    "  ds = tf.data.TFRecordDataset(os.path.join(root_dir, 'Airfoil/'+split+'.tfrecord'))\n",
    "  ds = ds.map(functools.partial(_parse, meta=meta), num_parallel_calls=8)\n",
    "  ds = ds.prefetch(1)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82b8b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data to a list variable 'l'\n",
    "ds = load_dataset('test')\n",
    "ds = ds.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "l = list(ds.prefetch(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "824e06ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The length of data is:\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a80e1",
   "metadata": {},
   "source": [
    "### Utility functions \n",
    "\n",
    "Here we define the functions that are needed for assisting in data processing.\n",
    "\n",
    "triangle_to_edges:  decomposes 2D triangular meshes to edges and returns the undirected graph nodes. \n",
    "\n",
    "NodeType: is subclass of enum with unique and unchanging integer valued attributes over instances in order to make sure values are unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc7fb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions, provided in the release of the code from the original MeshGraphNets study:\n",
    "#https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "\n",
    "def triangles_to_edges(faces):\n",
    "  \"\"\"Computes mesh edges from triangles.\n",
    "     Note that this triangles_to_edges method was provided as part of the\n",
    "     code release for the MeshGraphNets paper by DeepMind, available here:\n",
    "     https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "  \"\"\"\n",
    "  # collect edges from triangles\n",
    "  edges = tf.concat([faces[:, 0:2],\n",
    "                     faces[:, 1:3],\n",
    "                     tf.stack([faces[:, 2], faces[:, 0]], axis=1)], axis=0)\n",
    "  # those edges are sometimes duplicated (within the mesh) and sometimes\n",
    "  # single (at the mesh boundary).\n",
    "  # sort & pack edges as single tf.int64\n",
    "  receivers = tf.reduce_min(edges, axis=1)\n",
    "  senders = tf.reduce_max(edges, axis=1)\n",
    "  packed_edges = tf.bitcast(tf.stack([senders, receivers], axis=1), tf.int64)\n",
    "  # remove duplicates and unpack\n",
    "  unique_edges = tf.bitcast(tf.unique(packed_edges)[0], tf.int32)\n",
    "  senders, receivers = tf.unstack(unique_edges, axis=1)\n",
    "  # create two-way connectivity\n",
    "  return (tf.concat([senders, receivers], axis=0),\n",
    "          tf.concat([receivers, senders], axis=0))\n",
    "\n",
    "\n",
    "\n",
    "class NodeType(enum.IntEnum):\n",
    "    \"\"\"\n",
    "    Define the code for the one-hot vector representing the node types.\n",
    "    Note that this is consistent with the codes provided in the original\n",
    "    MeshGraphNets study: \n",
    "    https://github.com/deepmind/deepmind-research/tree/master/meshgraphnets\n",
    "    \"\"\"\n",
    "    NORMAL = 0\n",
    "    OBSTACLE = 1\n",
    "    AIRFOIL = 2\n",
    "    HANDLE = 3\n",
    "    INFLOW = 4\n",
    "    OUTFLOW = 5\n",
    "    WALL_BOUNDARY = 6\n",
    "    SIZE = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4aafc5",
   "metadata": {},
   "source": [
    "### Represent Features to create torch graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86731433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory:  0\n",
      "Trajectory:  1\n",
      "Trajectory:  2\n",
      "Trajectory:  3\n",
      "Trajectory:  4\n"
     ]
    }
   ],
   "source": [
    "#number of trajectories to train on.\n",
    "number_trajectories = 5\n",
    "\n",
    "#Splitting the dataset list to list of 100 trajectories of 601 trajectory length!\n",
    "data = [l[:601+i*601] for i in range(100)]\n",
    "\n",
    "# The time interval is 0.01s\n",
    "dt = 0.01\n",
    "\n",
    "#data_list consists of all the pytorch graph data of \n",
    "data_list = []\n",
    "for i in range(100):\n",
    "    \n",
    "    if(i==number_trajectories):\n",
    "        break\n",
    "    print(\"Trajectory: \",i)\n",
    "\n",
    "    #We iterate over all the time steps to produce an example graph except\n",
    "    #for the last one, which does not have a following time step to produce\n",
    "    #node output values\n",
    "    for ts in range(600):\n",
    "        #Get node features\n",
    "        #Note that it's faster to convert to numpy then to torch than to\n",
    "        #import to torch from h5 format directly\n",
    "        \n",
    "        \n",
    "        # Concat velocity and node type to construct node features in pytorch tensor!\n",
    "        momentum = torch.tensor(np.array(data[i][ts]['velocity']))\n",
    "        node_type = torch.tensor(np.array(tf.one_hot(data[i][0]['node_type'], NodeType.SIZE))).squeeze(1)\n",
    "        x = torch.cat((momentum,node_type),dim=-1).type(torch.float)\n",
    "        \n",
    "        \n",
    "        # Get edge indices in torch tensor!\n",
    "        b = data[i][ts]['cells']\n",
    "        #look at function triangles_to_edges\n",
    "        edges = triangles_to_edges(tf.convert_to_tensor(np.array(b)))\n",
    "#         print(edges)\n",
    "        edge_index = torch.cat( (torch.tensor(edges[0].numpy()).unsqueeze(0) ,\n",
    "                     torch.tensor(edges[1].numpy()).unsqueeze(0)), dim=0).type(torch.long)\n",
    "#         print(edge_index[0])\n",
    "\n",
    "        # Get edge features       \n",
    "        # Edge feature for each node pairs in edge_index\n",
    "        u_i=torch.tensor(np.array(data[i][ts]['mesh_pos']))[edge_index[0]]\n",
    "        u_j=torch.tensor(np.array(data[i][ts]['mesh_pos']))[edge_index[1]]\n",
    "        u_ij=u_i-u_j\n",
    "        u_ij_norm = torch.norm(u_ij,p=2,dim=1,keepdim=True)\n",
    "        edge_attr = torch.cat((u_ij,u_ij_norm),dim=-1).type(torch.float)\n",
    "\n",
    "        #Node outputs, for training (velocity)\n",
    "        v_t=torch.tensor(np.array(data[i][ts]['velocity']))\n",
    "        v_tp1=torch.tensor(np.array(data[i][ts+1]['velocity']))\n",
    "        y=((v_tp1-v_t)/dt).type(torch.float)\n",
    "\n",
    "        #Node outputs, for testing integrator (pressure)\n",
    "        p=torch.tensor(np.array(data[i][ts]['pressure']))\n",
    "        #Node outputs, for density\n",
    "        d=torch.tensor(np.array(data[i][ts]['density']))\n",
    "\n",
    "        #Data needed for visualization code\n",
    "        cells=torch.tensor(np.array(data[i][ts]['cells']))\n",
    "        mesh_pos=torch.tensor(np.array(data[i][ts]['mesh_pos']))\n",
    "\n",
    "        data_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr,y=y,p=p,\n",
    "                              cells=cells,mesh_pos=mesh_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5611ce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11523c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
